{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YBWtoq-Wqog"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Leitura da base\n",
        "with open(\"/content/drive/MyDrive/Whatsapp_NLP/_chat.txt\", 'r', encoding='utf-8') as file:\n",
        "    chat_data = file.readlines()\n",
        "\n",
        "\n",
        "# Função para analisar cada linha do arquivo de chat\n",
        "def parse_line(line):\n",
        "    # Expressão regular para corresponder à data e hora\n",
        "    match = re.match(r'\\[(\\d{2}/\\d{2}/\\d{2}), (\\d{2}:\\d{2}:\\d{2})\\] (.*?): (.*)', line)\n",
        "    if match:\n",
        "        date, time, user, message = match.groups()\n",
        "        datetime_str = date + ' ' + time\n",
        "        datetime_obj = datetime.strptime(datetime_str, '%d/%m/%y %H:%M:%S')\n",
        "        return datetime_obj, user, message\n",
        "    else:\n",
        "        return None, None, line\n",
        "\n",
        "# Analise os dados do chat\n",
        "parsed_data = [parse_line(line) for line in chat_data]\n",
        "\n",
        "# Crie um DataFrame\n",
        "chat_df = pd.DataFrame(parsed_data, columns=['datetime', 'user', 'message'])\n",
        "\n",
        "# Exiba as primeiras linhas do DataFrame\n",
        "chat_df.head()\n",
        "\n",
        "# Reamostre os dados para obter o número de mensagens por dia\n",
        "messages_per_day = chat_df.resample('D', on='datetime').size()\n",
        "\n",
        "max(messages_per_day)\n",
        "\n",
        "# Plote o número de mensagens por dia\n",
        "plt.figure(figsize=(12, 6))\n",
        "messages_per_day.plot()\n",
        "plt.title('Número de Mensagens por Dia')\n",
        "plt.xlabel('Data')\n",
        "plt.ylabel('Número de Mensagens')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Conte o número de mensagens enviadas por cada usuário\n",
        "user_message_counts = chat_df['user'].value_counts()\n",
        "\n",
        "# Plote o número de mensagens enviadas por cada usuário\n",
        "plt.figure(figsize=(12, 6))\n",
        "user_message_counts.plot(kind='bar')\n",
        "plt.title('Número de Mensagens por Usuário')\n",
        "plt.xlabel('Usuário')\n",
        "plt.ylabel('Número de Mensagens')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Categorize as mensagens\n",
        "def categorize_message(message):\n",
        "    if 'sticker omitted' in message:\n",
        "        return 'Sticker'\n",
        "    elif 'image omitted' in message:\n",
        "        return 'Imagem'\n",
        "    elif 'GIF omitted' in message:\n",
        "        return 'GIF'\n",
        "    elif 'video omitted' in message:\n",
        "        return 'Vídeo'\n",
        "    elif 'audio omitted' in message:\n",
        "        return 'Áudio'\n",
        "    elif 'document omitted' in message:\n",
        "        return 'Documento'\n",
        "    else:\n",
        "        return 'Texto'\n",
        "\n",
        "# Aplique a função para categorizar cada mensagem\n",
        "chat_df['message_type'] = chat_df['message'].apply(categorize_message)\n",
        "\n",
        "# Conte o número de cada tipo de mensagem\n",
        "message_type_counts = chat_df['message_type'].value_counts()\n",
        "\n",
        "# Plote a distribuição dos tipos de mensagens\n",
        "plt.figure(figsize=(12, 6))\n",
        "message_type_counts.plot(kind='bar')\n",
        "plt.title('Distribuição dos Tipos de Mensagens')\n",
        "plt.xlabel('Tipo de Mensagem')\n",
        "plt.ylabel('Número de Mensagens')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# import ace_tools as tools\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Baixar as stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "additional_stopwords = {'kkkk', 'kkkkk', 'kkkkkk', 'kkkkkkk', 'kkkkkkkk', 'kkkkkkkkk', 'kkkkkkkkkk',\n",
        "                        'vai', 'vou', 'cara', 'vou', 'eae', 'eai', 'vc','pra','aí','tá','lá', 'nao',\n",
        "                        'q','então','pq','n','ta', 'sim', 'aqui', 'pro', 'mto', 'eh', '@5561996886412',\n",
        "                        '@5561998118003','@5561998660927','@5561999011118', '@5561998669648','@5561983404441',\n",
        "                        '@5561998172241','@5561981154414','@5561991074343', 'acho', 'boa', 'qm', 'tbm'}\n",
        "stop_words.update(additional_stopwords)\n",
        "\n",
        "\n",
        "# Leitura da base\n",
        "with open(\"/content/drive/MyDrive/Whatsapp_NLP/_chat.txt\", 'r', encoding='utf-8') as file:\n",
        "    chat_data = file.readlines()\n",
        "\n",
        "\n",
        "# Função para analisar cada linha do arquivo de chat\n",
        "def parse_line(line):\n",
        "    match = re.match(r'\\[(\\d{2}/\\d{2}/\\d{2}), (\\d{2}:\\d{2}:\\d{2})\\] (.*?): (.*)', line)\n",
        "    if match:\n",
        "        date, time, user, message = match.groups()\n",
        "        datetime_str = date + ' ' + time\n",
        "        datetime_obj = datetime.strptime(datetime_str, '%d/%m/%y %H:%M:%S')\n",
        "        return datetime_obj, message\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Analise os dados do chat\n",
        "parsed_data = [parse_line(line) for line in chat_data]\n",
        "parsed_data = [data for data in parsed_data if data[0] is not None]\n",
        "\n",
        "# Crie um DataFrame\n",
        "chat_df = pd.DataFrame(parsed_data, columns=['datetime', 'message'])\n",
        "\n",
        "# Remover mensagens que contêm \"omitted\"\n",
        "chat_df = chat_df[~chat_df['message'].str.contains('omitted', na=False)]\n",
        "chat_df = chat_df[~chat_df['message'].str.contains('This message was deleted', na=False)]\n",
        "\n",
        "# Converter a coluna datetime para objeto datetime\n",
        "chat_df['datetime'] = pd.to_datetime(chat_df['datetime'])\n",
        "\n",
        "# Ordenar o DataFrame por datetime\n",
        "chat_df = chat_df.sort_values(by='datetime')\n",
        "\n",
        "# Calcular a diferença de tempo entre as mensagens\n",
        "chat_df['time_diff'] = chat_df['datetime'].diff()\n",
        "\n",
        "# Definir um novo grupo se a diferença de tempo for superior a duas horas\n",
        "chat_df['new_group'] = chat_df['time_diff'] > pd.Timedelta(hours=2)\n",
        "\n",
        "# Criar um identificador de grupo cumulativo\n",
        "chat_df['group_id'] = chat_df['new_group'].cumsum()\n",
        "\n",
        "# Concatenar mensagens dentro de cada grupo\n",
        "grouped_df = chat_df.groupby('group_id').agg({\n",
        "    'datetime': 'first',  # Manter a primeira data e hora do grupo\n",
        "    'message': ' '.join  # Concatenar mensagens\n",
        "}).reset_index()\n",
        "\n",
        "# Preprocessar o texto e remover stopwords\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', str(text))  # Remove caracteres não alfanuméricos\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)  # Remove múltiplos espaços\n",
        "    text = text.lower()  # Converte para minúsculas\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words and not word.startswith('5561') and not word.startswith('https')]  # Remove stopwords\n",
        "    return ' '.join(words)\n",
        "\n",
        "grouped_df['cleaned_message'] = grouped_df['message'].apply(preprocess_text)\n",
        "\n",
        "# Filtrar grupos com mais de 100 palavras\n",
        "grouped_df['word_count'] = grouped_df['cleaned_message'].apply(lambda x: len(x.split()))\n",
        "filtered_df = grouped_df[grouped_df['word_count'] > 50]\n",
        "\n",
        "# Concatenar todas as mensagens para análise de frequência\n",
        "all_words = ' '.join(filtered_df['cleaned_message']).split()\n",
        "\n",
        "# Contar a frequência das palavras\n",
        "word_freq = Counter(all_words)\n",
        "\n",
        "# Converter para DataFrame para visualização\n",
        "word_freq_df = pd.DataFrame(word_freq.items(), columns=['word', 'frequency']).sort_values(by='frequency', ascending=False)\n",
        "\n",
        "# Mostrar as 20 palavras mais frequentes\n",
        "top_words = word_freq_df.head(20)\n",
        "print(top_words)\n",
        "\n",
        "# Plotar as 20 palavras mais frequentes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(top_words['word'], top_words['frequency'], color='purple')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Words')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Verificar o número de grupos após o filtro\n",
        "print(f\"Número de grupos com mais de 100 palavras: {filtered_df.shape[0]}\")\n",
        "print(f\"Número de grupos totais: {grouped_df.shape[0]}\")\n",
        "\n",
        "# Salvar o DataFrame filtrado para a próxima etapa\n",
        "filtered_df.to_csv('filtered_grouped_chat.csv', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "\n",
        "# Carregar o DataFrame filtrado\n",
        "filtered_df = pd.read_csv('/content/filtered_grouped_chat.csv')\n",
        "\n",
        "# Utilizar modelo de linguagem pré-treinado para gerar embeddings\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embeddings = model.encode(filtered_df['cleaned_message'], show_progress_bar=True)\n",
        "\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "n_top = 15\n",
        "\n",
        "# Reduzir a dimensionalidade dos embeddings com UMAP\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine')\n",
        "umap_embeddings = umap_model.fit_transform(embeddings)\n",
        "\n",
        "# Ajustar parâmetros do BERTopic\n",
        "topic_model = BERTopic(\n",
        "    nr_topics=n_top,\n",
        "    language=\"portuguese\",\n",
        "    min_topic_size=5,\n",
        "    n_gram_range=(1, 3),\n",
        "    calculate_probabilities=True\n",
        ")\n",
        "\n",
        "topics, probabilities = topic_model.fit_transform(filtered_df['cleaned_message'], umap_embeddings)\n",
        "\n",
        "# Adicionar a classificação dos tópicos ao DataFrame\n",
        "filtered_df['topic'] = topics\n",
        "\n",
        "# Visualizar os tópicos\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(f\"{n_top} Tópicos mais discutidos no grupo\")\n",
        "print(topic_info)\n",
        "\n",
        "# Visualizar as palavras-chave de cada tópico\n",
        "for topic in range(len(topic_model.get_topics())):\n",
        "    if topic == -1:  # Outliers\n",
        "        continue\n",
        "    print(f\"Tópico {topic}:\")\n",
        "    print(topic_model.get_topic(topic))\n",
        "\n",
        "# Obter embeddings dos tópicos\n",
        "topic_embeddings = topic_model.topic_embeddings_\n",
        "\n",
        "# Agrupar tópicos utilizando clustering hierárquico\n",
        "clustering_model = AgglomerativeClustering(n_clusters=5)\n",
        "topic_labels = clustering_model.fit_predict(topic_embeddings)\n",
        "\n",
        "# Adicionar os rótulos de cluster aos tópicos\n",
        "topic_info['cluster'] = topic_labels\n",
        "\n",
        "# Visualizar os clusters de tópicos\n",
        "print(\"Clusters dos topicos\")\n",
        "print(topic_info)\n",
        "\n",
        "# Visualizar os tópicos em um gráfico interativo (opcional)\n",
        "topic_model.visualize_topics()\n",
        "\n",
        "# Visualizar a distribuição dos tópicos ao longo das mensagens (opcional)\n",
        "topic_model.visualize_distribution(probabilities[0])\n",
        "\n",
        "# Salvar o DataFrame resultante em um novo CSV\n",
        "filtered_df.to_csv('filtered_grouped_chat_with_topics.csv', index=False)\n",
        "topic_info.to_csv('topic_info_with_clusters.csv', index=False)\n",
        "\n",
        "# Exibir o DataFrame resultante\n",
        "# print(filtered_df.head(10))  # Exibir as primeiras 10 linhas do DataFrame\n",
        "# print(topic_info.head(10))  # Exibir as primeiras 10 linhas do DataFrame de tópicos\n",
        "\n",
        "#for i in range(n_top):\n",
        "    #filtered_df[filtered_df['topic'] == i].to_csv(f'filtered_grouped_chat_with_topics_{i+1}.csv', index=False)\n",
        "\n",
        "# Exibir o DataFrame resultante\n",
        "# print(filtered_df.head(10))  # Exibir as primeiras 10 linhas do DataFrame\n"
      ]
    }
  ]
}